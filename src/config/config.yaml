# --- Main model for single-model runs ---
# This is now primarily for standard (non-ensemble) runs or as a fallback.
# The ensemble mode will use the model names defined in the 'model_configs' section below.
# model_name: "meta-llama/Llama-3.2-3B"
# model_name: "mistralai/Mistral-7B-Instruct-v0.3"
# model_name: "google/gemma-1.1-7b-it"
# model_name: "meta-llama/Llama-3.1-8B"
model_name: "Qwen/Qwen3-8B"  # Testing Qwen3-8B alone first

# --- NEW: Ensemble Configuration ---
# This section controls the multi-model ensemble mode.
ensemble:
  enabled: true            # 4-model ensemble with Qwen3-8B
  batched: true            # NEW: process all queries per model before switching (fast path in main.py)
  preload_models: false    # (optional/future) preload all models at startup; keep false unless you implement it
  # List of model keys to use in the ensemble. These keys must map to an entry in 'model_configs'.
  models:
    - "llama-3.1-8b"
    - "mistral-7b"
    - "gemma-1.1-7b"
    - "qwen3-8b"  # NEW: 4th model for architecture diversity (Alibaba/Qwen)
  fusion_strategy: "error_aware"  # Options: 'error_aware', 'confidence', 'majority_vote'

# --- NEW: Model-Specific Configurations ---
# Centralizes the model Hugging Face IDs and per-model options.
# Minimal important update: per-model few_shot_examples_path so ensemble runs
# behave the same as single-model runs when few-shots are desired for DROP.
model_configs:
  "llama-3.1-8b":
    model_name: "meta-llama/Llama-3.1-8B"
    few_shot_examples_path: "data/drop_few_shot_examples.json"   # used for DROP when enabled
    # rules_file: "data/rules_drop.json"  # (optional per-model override; leave commented)
  "mistral-7b":
    model_name: "mistralai/Mistral-7B-Instruct-v0.3"
    few_shot_examples_path: "data/drop_few_shot_examples.json"
    # rules_file: "data/rules_drop.json"
  "gemma-1.1-7b":
    model_name: "google/gemma-1.1-7b-it"
    few_shot_examples_path: "data/drop_few_shot_examples.json"
    # rules_file: "data/rules_drop.json"
  "gemma-2-9b":
    model_name: "google/gemma-2-9b-it"
    few_shot_examples_path: "data/drop_few_shot_examples.json"
    # rules_file: "data/rules_drop.json"
  "qwen3-8b":
    model_name: "Qwen/Qwen3-8B"
    few_shot_examples_path: "data/drop_few_shot_examples.json"
    # rules_file: "data/rules_drop.json"

# (The rest of the configuration file remains the same)

data_dir: "data/"  # General data directory

# ---- Dataset Specific Paths ----
hotpotqa_dataset_path: "data/hotpot_dev_distractor_v1.json"
drop_dataset_path: "data/drop_dataset_dev.json"
squad_dataset_path: "data/SQuAD-2.0.json"  # Can also use ensemble_symrag/data/SQuAD-2.0.json from parent dir
drop_few_shot_examples_path: "data/drop_few_shot_examples.json"  # Global path for few-shot examples
squad_few_shot_examples_path: "data/squad_few_shot_examples.json"  # Optional few-shot for SQuAD

# ---- Feature Flags ----
# Control for including few-shot examples in NeuralRetriever for DROP
# 1 for true (include), 0 for false (exclude)
use_drop_few_shots: 1  # <-- minimal change: enable few-shots

# Rule Files
# The main system run (run_hysym_system) will dynamically create and prioritize rules_drop_dynamic.json for DROP.
# Ablation studies can explicitly choose between these via ablation_config.yaml.
hotpotqa_rules_file: "data/rules_hotpotqa.json"
drop_rules_file: "data/rules_drop.json"                     # Static/fallback DROP rules
drop_rules_dynamic_file: "data/rules_drop_dynamic_cleaned.json"     # Path where dynamic DROP rules are saved (cleaned, 1030 valid rules)
squad_rules_file: "data/rules_squad.json"                  # SQuAD 2.0 rules for reading comprehension
empty_rules_file: "data/empty_rules.json"                   # For "no rules" ablation (optional)

knowledge_base: "data/small_knowledge_base.txt"  # Ensured path consistency

# Dimension Management and Embedding Configuration
embeddings:
  symbolic_dim: 384
  neural_dim: 768
  target_dim: 768
  model_name: "all-MiniLM-L6-v2"

# Alignment Configuration
alignment:
  target_dim: 768
  num_heads: 4
  dropout: 0.1
  sym_dim: 384
  neural_dim: 768

# Basic resource thresholds (can be overridden by resource_config.yaml)
resource_thresholds:
  cpu:
    base_threshold: 0.85
    adjustment_factor: 0.08
  memory:
    base_threshold: 0.85
    adjustment_factor: 0.08
  gpu:
    base_threshold: 0.95
    adjustment_factor: 0.05

# Chunking Configuration
chunking:
  chunk_size: 512
  chunk_overlap: 128
  min_chunk_size: 64
  fallback_enabled: true

# Error Recovery Configuration
error_recovery:
  max_retries: 3
  backoff_factor: 1.5
  fallback_enabled: true
  cache_ttl: 3600  # Cache TTL for HybridIntegrator query cache

# Metrics Configuration (for MetricsCollector during standard runs)
metrics:
  detailed_logging: true
  save_frequency: 100        # How often MetricsCollector saves its internal state for standard runs
  history_window: 1000
  performance_tracking: true

# Symbolic Reasoner specific defaults (can be overridden by ablation configs)
# These are illustrative; your ablation_config.yaml would be the primary source for ablation-specific values
symbolic_match_threshold_hotpot: 0.1
symbolic_max_hops_hotpot: 5
symbolic_match_threshold_drop: 0.1
symbolic_max_hops_drop: 3
drop_rule_min_support: 5  # For dynamic rule extraction

# Neural Retriever specific defaults (can be overridden by ablation configs)
neural_use_quantization: True
neural_max_context_length: 2048
neural_chunk_size: 512
neural_overlap: 128

# System Control Manager defaults
error_retry_limit: 2
max_query_time: 30.0  # General default, might be adjusted per dataset in logic
